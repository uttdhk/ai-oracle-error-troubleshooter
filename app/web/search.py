# app/web/search.py
from __future__ import annotations
from typing import List, Dict, Optional, Tuple
import os
import re

from urllib.parse import urlparse, parse_qs, unquote

STRICT_ORA_MATCH = (os.getenv("STRICT_ORA_MATCH", "true").lower() == "true")

# ÏûÖÎ†• Î¨∏ÏûêÏó¥(text)ÏóêÏÑú Oracle Ïò§Î•ò ÏΩîÎìú(Ïòà: ORA-12514)Î•º Ï∞æÏïÑÎÇ¥Îäî ÎÇ¥Î∂Ä Ïú†Ìã∏Î¶¨Ìã∞ Ìï®Ïàò
def _extract_ora_code(text: str) -> str | None:
    if not text:
        return None
    m = re.search(r"\bORA-\d{5}\b", text.upper())
    return m.group(0) if m else None

# Ïõπ Í≤ÄÏÉâ Í≤∞Í≥º(hit) Í∞Ä ÌäπÏ†ï Oracle ÏóêÎü¨ ÏΩîÎìú(Ïòà: ORA-12514)Î•º Ïã§Ï†úÎ°ú Ìè¨Ìï®ÌïòÍ≥† ÏûàÎäîÏßÄ Ïó¨Î∂ÄÎ•º Í≤ÄÏÇ¨ÌïòÎäî Ìï®Ïàò
def _hit_contains_code(code: str, *, url: str = "", title: str = "", text: str = "", snippet: str = "") -> bool:
    """Í≤∞Í≥º(Î≥∏Î¨∏/Ï†úÎ™©/URL/Ïä§ÎãàÌé´)Ïóê ORA-ÏΩîÎìúÍ∞Ä Ïã§Ï†úÎ°ú Ìè¨Ìï®ÎêòÎäîÏßÄ Í≤ÄÏÇ¨"""
    if not code:
        return True
    hay = " ".join([url or "", title or "", text or "", snippet or ""]).upper()
    return code in hay

# DuckDuckGo(DDG) Í≤ÄÏÉâ Í≤∞Í≥º URLÏù¥ Ï§ëÍ∞Ñ Î¶¨ÎîîÎ†âÏÖò(https://duckduckgo.com/l/?uddg=...)ÏúºÎ°ú Í∞êÏã∏Ï†∏ ÏûàÏùÑ Îïå, Í∑∏ Ïã§Ï†ú ÏõêÎ≥∏ URLÏùÑ Ï∂îÏ∂ú(unwrap) ÌïòÎäî ÎÇ¥Î∂Ä Ïú†Ìã∏Î¶¨Ìã∞ Ìï®Ïàò
def _unwrap_ddg_redirect(href: str) -> str:
    """
    DDG HTML Í≤ÄÏÉâ Í≤∞Í≥ºÏùò redirect ÎßÅÌÅ¨(duckduckgo.com/l/?uddg=...)Î•º Ïã§Ï†ú Î™©Ï†ÅÏßÄÎ°ú ÌíÄÏñ¥Ï§ÄÎã§.
    redirectÍ∞Ä ÏïÑÎãàÎ©¥ ÏõêÎ≥∏ hrefÎ•º Í∑∏ÎåÄÎ°ú Î∞òÌôò.
    """
    try:
        if not href:
            return href
        u = urlparse(href)
        if (u.hostname or "").lower().endswith("duckduckgo.com") and u.path.startswith("/l/"):
            qs = parse_qs(u.query or "")
            if "uddg" in qs and qs["uddg"]:
                return unquote(qs["uddg"][0])
        return href
    except Exception:
        return href
    
# ---- ÌôòÍ≤Ω ÌîåÎûòÍ∑∏ ----
WEB_SEARCH_BACKEND = (os.getenv("WEB_SEARCH_BACKEND") or "").lower().strip()  # ddgs | duckduckgo_search | html | ""
INSECURE_SKIP_VERIFY = (os.getenv("INSECURE_SKIP_VERIFY", "false").lower() == "true")
CA_BUNDLE = os.getenv("REQUESTS_CA_BUNDLE") or os.getenv("SSL_CERT_FILE")  # ÏûàÏúºÎ©¥ requests verifyÏóê ÏÇ¨Ïö©

# ---- Ïö∞ÏÑ†ÏàúÏúÑ: ddgs(9.x) -> duckduckgo_search(6.x) -> html ÌååÏÑú ----
_have_ddgs = False
_have_ddgsearch = False
try:
    from ddgs import DDGS  # ddgs 9.x
    _have_ddgs = True
except Exception:
    pass

if not _have_ddgs:
    try:
        from duckduckgo_search import DDGS as LEGACY_DDGS  # 6.x
        _have_ddgsearch = True
    except Exception:
        pass
else:
    # ddgs ÏûàÎçîÎùºÎèÑ duckduckgo_searchÎèÑ ÏûàÏúºÎ©¥ Í∏∞Î°ùÌï¥Îë†(Ìè¥Î∞±Ïö©)
    try:
        from duckduckgo_search import DDGS as LEGACY_DDGS  # 6.x
        _have_ddgsearch = True
    except Exception:
        pass


# ---- Allowlist (Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ) ----
_ALLOWED = {
    "oracle.com",              # ‚úÖ *.oracle.com Ï†ÑÏó≠ ÌóàÏö©
    "docs.oracle.com",
    "asktom.oracle.com",
    "community.oracle.com",
    "oracle-base.com",
    "stackoverflow.com",
    "dba.stackexchange.com",
    "github.com",
    "medium.com",
    "blogspot.com",
}

# Ï£ºÏñ¥ÏßÑ URLÏùò Ìò∏Ïä§Ìä∏(host, ÎèÑÎ©îÏù∏) Í∞Ä ÌóàÏö© Í∞ÄÎä•Ìïú(‚ÄúÏã†Î¢∞Ìï† Ïàò ÏûàÎäî‚Äù) ÎèÑÎ©îÏù∏Ïù∏ÏßÄ Í≤ÄÏÇ¨ÌïòÎäî ÎÇ¥Î∂Ä Ìï®Ïàò
def _host_ok(url: str) -> bool:
    from urllib.parse import urlparse
    host = (urlparse(url).hostname or "").lower()
#    return any(host.endswith(d) for d in _ALLOWED)
    # *.oracle.com Ï†ÑÏ≤¥ ÌóàÏö© + ÎÇòÎ®∏ÏßÄÎäî endswith Ï≤¥ÌÅ¨
    return (
        host.endswith(".oracle.com") or host == "oracle.com" or
        any(host.endswith(d) for d in _ALLOWED)
    )

MIN_LEN_PRIMARY   = 220
MIN_LEN_SECONDARY = 60

# ---- HTML ÌååÏÑú Î∞±ÏóÖ (requests + BeautifulSoup; verify/ÌîÑÎ°ùÏãú ÏûêÎèô) ----
# Ïõπ Í≤ÄÏÉâÏùÑ Ìïú Î≤à ÏàòÌñâÌï¥ÏÑú(once) Í∑∏ Í≤∞Í≥ºÎ•º HTML ÌòïÌÉúÎ°ú Í∞ÄÏ†∏Ïò§Îäî Ìï®Ïàò
def _search_once_html(query: str, max_results: int = 6, region: str = "wt-wt") -> List[Dict[str, str]]:
    items: List[Dict[str, str]] = []
    try:
        import requests
        from bs4 import BeautifulSoup
    except Exception:
        return items
    params = {"q": query, "kl": region}
    headers = {"User-Agent": "Mozilla/5.0", "Accept-Language": "en-US,en;q=0.9"}
    verify = False if INSECURE_SKIP_VERIFY else (CA_BUNDLE or True)
    try:
        r = requests.get("https://html.duckduckgo.com/html/", params=params, headers=headers,
                         timeout=12, verify=verify)
        if r.status_code != 200:
            return items
        soup = BeautifulSoup(r.text, "html.parser")
        for a in soup.select("a.result__a"):
            href_raw = (a.get("href") or "").strip()
            url = _unwrap_ddg_redirect(href_raw)  # ‚úÖ Î¶¨Îã§Ïù¥Î†âÌä∏ Ìï¥Ï†ú
            title = (a.get_text(strip=True) or url)
            if not url or not _host_ok(url):
                continue
            items.append({"title": title, "url": url, "snippet": ""})
            if len(items) >= max_results:
                break
    except Exception:
        return items
    return items

# ÏõπÌéòÏù¥ÏßÄÎ•º Í∞ÄÏ†∏(fetch) ÏôÄÏÑú, Í∑∏Ï§ë ÏÇ¨ÎûåÏù¥ ÏùΩÏùÑ Ïàò ÏûàÎäî(Î≥∏Î¨∏ Ï§ëÏã¨Ïùò) ÌÖçÏä§Ìä∏Îßå Ï∂îÏ∂ú(readable) ÌïòÎäî Ìï®Ïàò
def _fetch_readable(url: str) -> Optional[str]:
    # trafilatura Ïö∞ÏÑ† (ÎÇ¥Î∂Ä requests ÏÇ¨Ïö© Ïãú CA_BUNDLE/ÌîÑÎ°ùÏãú ÏûêÎèô Î∞òÏòÅ)
    try:
        import trafilatura
        # trafilaturaÎäî verify Ïù∏ÏûêÎ•º ÏßÅÏ†ë Î∞õÏßÄ ÏïäÏßÄÎßå, ÎÇ¥Î∂Ä ÏöîÏ≤≠ÏùÄ ÏãúÏä§ÌÖú CA/ÌîÑÎ°ùÏãú ÌôòÍ≤ΩÏùÑ Îî∞Î¶ÖÎãàÎã§.
        downloaded = trafilatura.fetch_url(url, timeout=12)
        if downloaded:
            text = trafilatura.extract(downloaded, include_comments=False, include_tables=False)
            if text and text.strip():
                return text.strip()
    except Exception:
        pass
    # requests + bs4 Î∞±ÏóÖ (verify Ï†úÏñ¥)
    try:
        import requests
        from bs4 import BeautifulSoup
    except Exception:
        return None
    headers = {"User-Agent": "Mozilla/5.0", "Accept-Language": "en-US,en;q=0.9"}
    verify = False if INSECURE_SKIP_VERIFY else (CA_BUNDLE or True)
    try:
        r = requests.get(url, headers=headers, timeout=12, verify=verify)
        if r.status_code != 200:
            return None
        soup = BeautifulSoup(r.text, "html.parser")
        for t in soup(["script", "style", "noscript"]):
            t.decompose()
        cands = []
        for sel in ("article", "[role=main]", "main", ".content", "#content"):
            for node in soup.select(sel):
                txt = node.get_text("\n", strip=True)
                if txt:
                    cands.append(txt)
        if not cands:
            cands.append(soup.get_text("\n", strip=True))
        cands.sort(key=len, reverse=True)
        txt = cands[0] if cands else ""
        return txt.strip() or None
    except Exception:
        return None

# Ïõπ Í≤ÄÏÉâÏùÑ Ìïú Î≤à ÏàòÌñâÌïòÎäî ÌïµÏã¨ Ìï®Ïàò
def _search_once(query: str, max_results: int = 6, region: str = "wt-wt") -> List[Dict[str, str]]:
    """
    Î∞±ÏóîÎìú ÏÑ†ÌÉù ÏàúÏÑú:
      1) WEB_SEARCH_BACKEND Î°ú Î™ÖÏãúÎêú ÏóîÏßÑ
      2) ddgs -> duckduckgo_search -> html (ÏûêÎèô Ìè¥Î∞±)
    ddgs/duckduckgo_searchÎäî verify Ï†úÏñ¥Í∞Ä Ïñ¥Î†§Ïö∞ÎØÄÎ°ú TLS ÏóêÎü¨ Ïãú ÏûêÎèô Ìè¥Î∞±Ìï©ÎãàÎã§.
    """
    # 1) Í∞ïÏ†ú Î∞±ÏóîÎìú
    backend = WEB_SEARCH_BACKEND
    items: List[Dict[str, str]] = []
    # ddgs Í∞ïÏ†ú
    if backend == "ddgs":
        try:
            if not _have_ddgs:
                raise RuntimeError("ddgs Î™®Îìà ÏóÜÏùå")
            from ddgs.exceptions import DDGSException  # type: ignore
            with DDGS() as s:
                for r in s.text(query, max_results=max_results, safesearch="moderate", region=region):
                    url = (r.get("href") or r.get("url") or "").strip()
                    title = (r.get("title") or "").strip() or url
                    if not url or not _host_ok(url):
                        continue
                    items.append({"title": title, "url": url, "snippet": r.get("body") or ""})
                    if len(items) >= max_results: break
            return items
        except Exception:
            return _search_once_html(query, max_results, region)
    # duckduckgo_search Í∞ïÏ†ú
    if backend == "duckduckgo_search":
        try:
            if not _have_ddgsearch:
                raise RuntimeError("duckduckgo_search Î™®Îìà ÏóÜÏùå")
            with LEGACY_DDGS() as s:
                for r in s.text(query, max_results=max_results, safesearch="moderate", region=region):
                    url = (r.get("href") or r.get("url") or "").strip()
                    title = (r.get("title") or "").strip() or url
                    if not url or not _host_ok(url):
                        continue
                    items.append({"title": title, "url": url, "snippet": r.get("body") or ""})
                    if len(items) >= max_results: break
            return items
        except Exception:
            return _search_once_html(query, max_results, region)
    # html Í∞ïÏ†ú
    if backend == "html":
        return _search_once_html(query, max_results, region)

    # 2) ÏûêÎèô(Ïö∞ÏÑ† ddgs ÏãúÎèÑ -> Ïã§Ìå® Ïãú duckduckgo_search -> Ïã§Ìå® Ïãú html)
    if _have_ddgs:
        try:
            from ddgs.exceptions import DDGSException  # type: ignore
            with DDGS() as s:
                for r in s.text(query, max_results=max_results, safesearch="moderate", region=region):
                    url = (r.get("href") or r.get("url") or "").strip()
                    title = (r.get("title") or "").strip() or url
                    if not url or not _host_ok(url):
                        continue
                    items.append({"title": title, "url": url, "snippet": r.get("body") or ""})
                    if len(items) >= max_results: break
            return items
        except Exception:
            # TLS / DDGSException -> Îã§Ïùå Î∞±ÏóîÎìú
            pass
    if _have_ddgsearch:
        try:
            with LEGACY_DDGS() as s:
                for r in s.text(query, max_results=max_results, safesearch="moderate", region=region):
                    url = (r.get("href") or r.get("url") or "").strip()
                    title = (r.get("title") or "").strip() or url
                    if not url or not _host_ok(url):
                        continue
                    items.append({"title": title, "url": url, "snippet": r.get("body") or ""})
                    if len(items) >= max_results: break
            return items
        except Exception:
            pass
    # ÏµúÌõÑ: html ÌååÏÑú
    return _search_once_html(query, max_results, region)

# Ïõπ Í≤ÄÏÉâÏóê ÏÇ¨Ïö©Ìï† ÏøºÎ¶¨(query) Î¨∏ÏûêÏó¥Îì§ÏùÑ ÏûêÎèô ÏÉùÏÑ±ÌïòÎäî Ìï®Ïàò
def _build_queries(user_query: str) -> List[str]:
    qs = [user_query]
    m = re.search(r"(ORA-\d{5})", (user_query or "").upper())
    if m:
        code  = m.group(1)
        short = code[:8]
        qs += [
            f'"{code}"',                            # Ï†ïÌôï Îß§Ïπ≠
            f'"{code}" Oracle',
            f'{code.replace("-", " ")} site:docs.oracle.com',
            f'"{code}" site:docs.oracle.com',
            f'"{code}" site:oracle-base.com',
            f'{short} Oracle error',
            'ORA- error code list site:docs.oracle.com',
            'list of ORA- codes oracle-base',
            f'"{code}" "does not exist" Oracle',
            f'"{code}" site:community.oracle.com',
            f'"{code}" site:asktom.oracle.com',
        ]
    else:
        qs += [
            f'{user_query} site:docs.oracle.com',
            f'{user_query} Oracle error',
        ]
    seen, out = set(), []
    for q in qs:
        if q not in seen:
            out.append(q); seen.add(q)
    return out

# Ïó¨Îü¨ Í≤ÄÏÉâ Í≤∞Í≥ºÎÇò Ï≤òÎ¶¨ Í≤∞Í≥ºÎ•º ‚ÄúÎ™®ÏïÑ(collect)‚Äù ÌïòÎÇòÏùò Î¶¨Ïä§Ìä∏ÎÇò ÎîïÏÖîÎÑàÎ¶¨ ÌòïÌÉúÎ°ú Ï†ïÎ¶¨ÌïòÎäî ÎÇ¥Î∂Ä Ïú†Ìã∏Î¶¨Ìã∞ Ìï®Ïàò
def _collect(queries: List[str], min_len: int, max_results: int = 6, *, code: str | None = None) -> List[Dict[str, str]]:
    collected: List[Dict[str, str]] = []
    seen_urls = set()
    for q in queries:
        for item in _search_once(q, max_results=max_results, region="wt-wt"):
            url = item["url"]
            if url in seen_urls:
                continue
            text = _fetch_readable(url)
            snippet = item.get("snippet") or ""
            title = item.get("title") or ""
            # üîí ÏóÑÍ≤© Îß§Ïπ≠: ORA ÏΩîÎìúÍ∞Ä Î≥∏Î¨∏/Ï†úÎ™©/URL/Ïä§ÎãàÌé´ Ïñ¥ÎîîÏóêÎèÑ ÏóÜÏúºÎ©¥ Î≤ÑÎ¶º
            if STRICT_ORA_MATCH and code:
                if not _hit_contains_code(code, url=url, title=title, text=(text or ""), snippet=snippet):
                    continue
            # Í∏∏Ïù¥Ïª∑ (ÏßßÏïÑÎèÑ Ïä§ÎãàÌé´Ïù¥ ÏûàÏúºÎ©¥ ÏàòÏö©)
            if text and len(text) > min_len:
                collected.append({"title": title or url, "url": url, "text": text})
                seen_urls.add(url)
            elif text:
                collected.append({"title": title or url, "url": url, "text": text})
                seen_urls.add(url)
            elif snippet:
                collected.append({"title": title or url, "url": url, "text": snippet})
                seen_urls.add(url)
    return collected

# Ïõπ Í≤ÄÏÉâÏùÑ ‚ÄúÏïàÏ†ÑÌïòÍ≤å(safely)‚Äù ÏàòÌñâÌïòÎäî Ìï®Ïàò
def search_web_safely(user_query: str, max_results: int = 6) -> Tuple[List[Dict[str, str]], List[str]]:
    queries = _build_queries(user_query)
    code = _extract_ora_code(user_query)  # ‚Üê Ï∂îÍ∞Ä

    primary = _collect(queries, min_len=MIN_LEN_PRIMARY, max_results=max_results, code=code)
    if primary:
        return primary, queries

    secondary = _collect(queries, min_len=MIN_LEN_SECONDARY, max_results=max_results, code=code)
    if secondary:
        return secondary, queries

    tertiary = _collect(queries, min_len=MIN_LEN_SECONDARY, max_results=max_results, code=code)
    return tertiary, queries
